#edit slurm.conf when updating proc or ram
  /admin/slurm/etc/slurm.conf

#reconfig slurm
  systemctl stop slurmd
  systemctl status slurmd
  #steps to get slurm running on node
  systemctl restart slurmd
  scontrol update nodename=[node hostname] state=idle
  scontrol reconfig 


#all the AI/PLAB IPs are in /etc/hosts
  wasup                       (shows job/queue information)
  scontrol show job <JOBID>         (show jobs)
  sinfo                     (information on the status of each computer on your local network.)
  bhosts/shosts                       (shows current status of nodes in the cluster)
  lshosts                       (shows host constraint/feature information)
  ssacli	                   HPE Command Line Smart Storage Administration Utility
  ssaducli	           HPE Command Line Smart Storage Administration Diagnostics
  ssa	                           HPE Array Smart Storage Administration Service

############## accounting use ############
sacct 
is used to report job or job step accounting information about active or completed jobs.
  sacct -u [username]

sstat
report accounting information about currently running jobs and job setps. (more detail than sacct)

sreport
report resources usage by cluster, partition, user, account
################ end ##################

salloc 
is used to allocate resources for a job in real time. Typically this is used to allocate resources and spawn a shell. 
The shell is then used to execute srun commands to launch parallel tasks.
  salloc -ntasks=[#] -time=[#] bash

sattach 
is used to attach standard input, output, and error plus signal capabilities to a currently running job or job step. 
One can attach to and detach from jobs multiple times.

  scontrol update nodename=[hostname] state=[idle|down] 

sbatch 
is used to submit a job script for later execution. The script will typically contain one or more srun commands to launch parallel tasks.
  sbatch -ntasks=[#] -time=[#] [bash_script]

sbcast 
is used to transfer a file from local disk to local disk on the nodes allocated to a job. This can be used to effectively use diskless 
compute nodes or provide improved performance relative to a shared file system.

scancel 
is used to cancel a pending or running job or job step. It can also be used to send an arbitrary signal to all processes associated 
with a running job or job step.

scontrol 
is the administrative tool used to view and/or modify Slurm state. Note that many scontrol commands can only be executed as user root.
  scontrol: update NodeName=cn114 State=RESUME
  scontrol update NodeName=node[02-04] State=DRAIN Reason=”Cloning”

sinfo 
reports the state of partitions and nodes managed by Slurm. It has a wide variety of filtering, sorting, and formatting options.
  sinfo --Node (report status in node-oriented form)
  sinfo -p debug (report status of nodes in partition "debug")
  
smap 
reports state information for jobs, partitions, and nodes managed by Slurm, but graphically displays the information to reflect 
network topology.

squeue 
reports the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. By default, 
it reports the running jobs in priority order and then the pending jobs in priority order.
  squeue -i60 (report status every 60 seconds)
  squeue -u [username] -t all (report jobs for user in any state)
  
srun 
is used to submit a job for execution or initiate job steps in real time. srun has a wide variety of options to specify resource 
requirements, including: minimum and maximum node count, processor count, specific nodes to use or not use, and specific node 
characteristics (so much memory, disk space, certain required features, etc.). A job can contain multiple job steps executing 
sequentially or in parallel on independent or shared resources within the job's node allocation.
  srun -ntasks[#] -[allocation]
  srun -nnodes=[#] --exclusive -[allocation]
  srun -label hostname
  
srun_cr
 wrapper to srun for support of berkeley checkpoint/restart

strigger 
is used to set, get or view event triggers. Event triggers include things such as nodes going down or jobs approaching their time limit.

sview 
is a graphical user interface to get and update state information for jobs, partitions, and nodes managed by Slurm.

############### management control command ##############
sacctmgr
database management tool
add/delete clusters, accounts, user
get/set resource limits, fair-share allocation

sprio
view factors comprising a job's priority

sshare
view current hierarchical fair-share info
################# end ####################

hpcgate
slurm
The path has:
  /opt/slurm/custom
  /opt/slurm/bin
  /opt/slurm/sbin
  /apps/slurm/custom
  /apps/slurm/bin
  /apps/slurm/sbin 
But /opt/slurm/custom -> /apps/slurm/custom

##############################
cat /etc/profile.d/slurm

#OG:
[root@hpcgate opt]# cat /etc/profile.d/slurm.sh
# SLURM initialization script (sh)
if [ -e /optlocal/slurm/custom/slurm.sh ]
then
  . /optlocal/slurm/custom/slurm.sh
fi

#NEW:
# SLURM initialization script (sh)
if [ -e /optlocal/aislurm/custom/slurm.sh ]
then
  . /optlocal/aislurm/custom/slurm.sh
fi
############ End ################
 
############################## Daemon ###########################
slurmctld - central controller (Monitors state of resources) (CMU)
  or /etc/init.d/slurm restart
slurmd - compute node daemon
slurmdbd - database daemon

-c clear previous state, purge ALL JOB, set, partition state
-C print the node's current configuration and exit (use with slurmd)
-D run in the foreground, logs are written to stdout
-v verbose error message (each "v" doubles volume of messages)

example:  
  slurmctld -Dvvv
  slurmd -Dcv
  
munge
Slurm's default authentication and digital signature, each node in a cluster is configured with a munge key.
munge generated credential:
-user ID
-group ID
-time stamp

#################################################
Node are in DOWN state:

1. Check the reason why the node is down using the command 
  scontrol show node <host_name> 
This will show the reason why the node was set down and the time when it happened. 
If there is insufficient disk space, memory space, etc. compared to the parameters specified in the slurm.conf file 
then either fix the node or change slurm.conf.

2. If the reason is "Not responding", then check communications between the control machine and the DOWN node using the command 
  ping <address>
being sure to specify the NodeAddr values configured in slurm.conf. 
If ping fails, then fix the network or addresses in slurm.conf.

3. Next, login to a node tha. Slurm considers to be in a DOWN state and check if the slurmd daemon is running with the command 
  ps -el | grep slurmd
If slurmd is not running, restart it (typically as user root using the command "/etc/init.d/slurm start"). 
You should check the log file (SlurmdLog in the slurm.conf file) for an indication of why it failed. 
You can get the status of the running slurmd daemon by executing the command "scontrol show slurmd" on the node of interest. 
Check the value of "Last slurmctld msg time" to determine if the slurmctld is able to communicate with the slurmd.

4. If slurmd is running but not responding (a very rare situation), then kill and restart it (typically as user root using the 
commands "/etc/init.d/slurm stop" and then "/etc/init.d/slurm start").
##################################################
