#edit slurm.conf when updating proc or ram
  /admin/slurm/etc/slurm.conf
  /apps/aislurm/etc/slurm.conf
  
# adding or removing nodes
# backup /apps/aislurm/etc/slurm.conf
  /apps/aislurm/etc/slurm.conf /apps/aislurm/etc/slurm.conf.[date]
  vi /apps/aislurm/etc/slurm.conf
# edit both 'COMPUTE NODES' and 'Partition'  
  systemctl restart slurmctl.service
 
#reconfig slurm
  systemctl stop slurmd
  systemctl status slurmd
  #steps to get slurm running on node
  systemctl restart slurmd
  scontrol update nodename=[node hostname] state=idle
  scontrol reconfig 


#all the AI/PLAB IPs are in /etc/hosts
  wasup                       (shows job/queue information)
  scontrol show job <JOBID>         (show jobs)
  sinfo                     (information on the status of each computer on your local network.)
  bhosts/shosts                       (shows current status of nodes in the cluster)
  lshosts                       (shows host constraint/feature information)
  ssacli	                   HPE Command Line Smart Storage Administration Utility
  ssaducli	           HPE Command Line Smart Storage Administration Diagnostics
  ssa	                           HPE Array Smart Storage Administration Service

############## accounting use ############
sacct 
is used to report job or job step accounting information about active or completed jobs.
  sacct -u [username]

sstat
report accounting information about currently running jobs and job setps. (more detail than sacct)

sreport
report resources usage by cluster, partition, user, account
################ end ##################

salloc 
is used to allocate resources for a job in real time. Typically this is used to allocate resources and spawn a shell. 
The shell is then used to execute srun commands to launch parallel tasks.
  salloc -ntasks=[#] -time=[#] bash

sattach 
is used to attach standard input, output, and error plus signal capabilities to a currently running job or job step. 
One can attach to and detach from jobs multiple times.

  scontrol update nodename=[hostname] state=[idle|down] 

sbatch 
is used to submit a job script for later execution. The script will typically contain one or more srun commands to launch parallel tasks.
  sbatch -ntasks=[#] -time=[#] [bash_script]

sbcast 
is used to transfer a file from local disk to local disk on the nodes allocated to a job. This can be used to effectively use diskless 
compute nodes or provide improved performance relative to a shared file system.

scancel 
is used to cancel a pending or running job or job step. It can also be used to send an arbitrary signal to all processes associated 
with a running job or job step.

scontrol 
is the administrative tool used to view and/or modify Slurm state. Note that many scontrol commands can only be executed as user root.
  scontrol: update NodeName=cn114 State=RESUME
  scontrol update NodeName=node[02-04] State=DRAIN Reason=”Cloning”

sinfo 
reports the state of partitions and nodes managed by Slurm. It has a wide variety of filtering, sorting, and formatting options.
  sinfo --Node (report status in node-oriented form)
  sinfo -p debug (report status of nodes in partition "debug")
  
smap 
reports state information for jobs, partitions, and nodes managed by Slurm, but graphically displays the information to reflect 
network topology.

squeue 
reports the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. By default, 
it reports the running jobs in priority order and then the pending jobs in priority order.
  squeue -i60 (report status every 60 seconds)
  squeue -u [username] -t all (report jobs for user in any state)
  
srun 
is used to submit a job for execution or initiate job steps in real time. srun has a wide variety of options to specify resource 
requirements, including: minimum and maximum node count, processor count, specific nodes to use or not use, and specific node 
characteristics (so much memory, disk space, certain required features, etc.). A job can contain multiple job steps executing 
sequentially or in parallel on independent or shared resources within the job's node allocation.
  srun -ntasks[#] -[allocation]
  srun -nnodes=[#] --exclusive -[allocation]
  srun -label hostname
  
srun_cr
 wrapper to srun for support of berkeley checkpoint/restart

strigger 
is used to set, get or view event triggers. Event triggers include things such as nodes going down or jobs approaching their time limit.

sview 
is a graphical user interface to get and update state information for jobs, partitions, and nodes managed by Slurm.

############### management control command ##############
sacctmgr
database management tool
add/delete clusters, accounts, user
get/set resource limits, fair-share allocation

sprio
view factors comprising a job's priority

sshare
view current hierarchical fair-share info
################# end ####################

hpcgate
slurm
The path has:
  /opt/slurm/custom
  /opt/slurm/bin
  /opt/slurm/sbin
  /apps/slurm/custom
  /apps/slurm/bin
  /apps/slurm/sbin 
But /opt/slurm/custom -> /apps/slurm/custom

##############################
cat /etc/profile.d/slurm

#OG:
[root@hpcgate opt]# cat /etc/profile.d/slurm.sh
# SLURM initialization script (sh)
if [ -e /optlocal/slurm/custom/slurm.sh ]
then
  . /optlocal/slurm/custom/slurm.sh
fi

#NEW:
# SLURM initialization script (sh)
if [ -e /optlocal/aislurm/custom/slurm.sh ]
then
  . /optlocal/aislurm/custom/slurm.sh
fi
############ End ################
 
############################## Daemon ###########################
slurmctld - central controller (Monitors state of resources) (CMU)
  or /etc/init.d/slurm restart
slurmd - compute node daemon
slurmdbd - database daemon

-c clear previous state, purge ALL JOB, set, partition state
-C print the node's current configuration and exit (use with slurmd)
-D run in the foreground, logs are written to stdout
-v verbose error message (each "v" doubles volume of messages)

example:  
  slurmctld -Dvvv
  slurmd -Dcv
  
munge
Slurm's default authentication and digital signature, each node in a cluster is configured with a munge key.
munge generated credential:
-user ID
-group ID
-time stamp

#################################################
Node are in DOWN state:

1. Check the reason why the node is down using the command 
  scontrol show node <host_name> 
This will show the reason why the node was set down and the time when it happened. 
If there is insufficient disk space, memory space, etc. compared to the parameters specified in the slurm.conf file 
then either fix the node or change slurm.conf.

2. If the reason is "Not responding", then check communications between the control machine and the DOWN node using the command 
  ping <address>
being sure to specify the NodeAddr values configured in slurm.conf. 
If ping fails, then fix the network or addresses in slurm.conf.

3. Next, login to a node tha. Slurm considers to be in a DOWN state and check if the slurmd daemon is running with the command 
  ps -el | grep slurmd
If slurmd is not running, restart it (typically as user root using the command "/etc/init.d/slurm start"). 
You should check the log file (SlurmdLog in the slurm.conf file) for an indication of why it failed. 
You can get the status of the running slurmd daemon by executing the command "scontrol show slurmd" on the node of interest. 
Check the value of "Last slurmctld msg time" to determine if the slurmctld is able to communicate with the slurmd.

4. If slurmd is running but not responding (a very rare situation), then kill and restart it (typically as user root using the 
commands "/etc/init.d/slurm stop" and then "/etc/init.d/slurm start").
##################################################





####################### Confiuring Slurm ##########################

# Slurm configuration:

# NIS already has a slurm users, it will cause an error so moving useradd or adduser and userdel or deluser temporary
# check to see which add and del bin file you have
  which adduser
  which useradd
  mv /usr/sbin/adduser /usr/sbin/adduser.bak
  mv /usr/sbin/useradd /usr/sbin/useradd.bak
  mv /usr/sbin/deluser /usr/sbin/deluser.bak
  mv /usr/sbin/userdel /usr/sbin/userdel.bak 
 
# create new useradd and userdel to return 0
  vi /usr/sbin/useradd
#################  
#!/bin/bash
exit 0
#################

# changing permission bit and copying a userdel
  chmod u+x /usr/sbin/useradd
  cp /usr/sbin/useradd /usr/sbin/userdel
  cp /usr/sbin/useradd /usr/sbin/deluser
  cp /usr/sbin/useradd /usr/sbin/adduser

# For whatever reason, the nodes have a munge user, but no munge group.
# Make sure there's a munge group and corresponding user:
  grep munge /etc/passwd
    munge:x:1002:1002::/nonexistent:/bin/sh
  grep munge /etc/group

# If there isn't a munge group, add a munge group by editing /etc/group and inserting:
  munge:x:1002:

# Install munge:
  apt install munge

# Copy /etc/munge/munge.key from another node:
  scp znode39:/etc/munge/munge.key /etc/munge

# Restart munge and make sure it's OK:
  systemctl restart munge

   root@znode40:~# systemctl status munge
   ● munge.service - MUNGE authentication service
      Loaded: loaded (/lib/systemd/system/munge.service; enabled; vendor preset: en
      Active: active (running) since Thu 2020-02-06 18:43:03 UTC; 3s ago
        Docs: man:munged(8)
     Process: 61545 ExecStart=/usr/sbin/munged (code=exited, status=0/SUCCESS)
    Main PID: 61554 (munged)
       Tasks: 4 (limit: 12287)
      CGroup: /system.slice/munge.service
              └─61554 /usr/sbin/munged

   Feb 06 18:43:03 znode40 systemd[1]: Starting MUNGE authentication service...
   Feb 06 18:43:03 znode40 systemd[1]: Started MUNGE authentication service.


# Install slurm:
  apt install slurm slurm-client slurmd

# Move adduser userdel back into place:
  mv /usr/sbin/useradd.bak /usr/sbin/useradd
  mv /usr/sbin/adduser.bak /usr/sbin/adduser
  mv /usr/sbin/userdel.bak /usr/sbin/userdel
  mv /usr/sbin/deluser.bak /usr/sbin/deluser
  
# Copy /etc/slurm-llnl/slurm.conf from another node:
  scp znode39:/etc/slurm-llnl/slurm.conf /etc/slurm-llnl/

# Edit /lib/systemd/system/slurmd.service and change
  vi /lib/systemd/system/slurmd.service
  
    PIDFile=/var/run/slurm-llnl/slurmd.pid
        to
    PIDFile=/var/run/slurmd.pid

# Reload & Restart slurmd and make sure it's OK:
  systemctl daemon-reload
  systemctl restart slurmd

  root@znode40:/usr/sbin# systemctl status slurmd
   ● slurmd.service - Slurm node daemon
      Loaded: loaded (/lib/systemd/system/slurmd.service; enabled; vendor preset: enabled)
      Active: active (running) since Thu 2020-02-06 19:06:31 UTC; 12s ago
        Docs: man:slurmd(8)
     Process: 68865 ExecStart=/usr/sbin/slurmd $SLURMD_OPTIONS (code=exited, status=0/SUCCESS)
    Main PID: 68867 (slurmd)
       Tasks: 1 (limit: 12287)
      CGroup: /system.slice/slurmd.service
              └─68867 /usr/sbin/slurmd

   Feb 06 19:06:31 znode40 systemd[1]: Starting Slurm node daemon...
   Feb 06 19:06:31 znode40 systemd[1]: slurmd.service: Can't open PID file /var/run/slurmd.pid (yet?) after sta
   Feb 06 19:06:31 znode40 systemd[1]: Started Slurm node daemon.


# Put the node online and make sure it's online:
  scontrol update node=znode40 state=idle

  sinfo
   PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
   short*       up    4:00:00      8 drain* znode[42,47-53]
   short*       up    4:00:00      2  down* znode[41,56]
   short*       up    4:00:00      2  alloc znode[44-45]
   short*       up    4:00:00      6   idle znode[39-40,43,46,54-55]
   normal       up 1-00:00:00      8 drain* znode[42,47-53]
   normal       up 1-00:00:00      2  down* znode[41,56]
   normal       up 1-00:00:00      2  alloc znode[44-45]
   normal       up 1-00:00:00      6   idle znode[39-40,43,46,54-55]
   long         up 7-00:00:00      8 drain* znode[42,47-53]
   long         up 7-00:00:00      2  down* znode[41,56]
   long         up 7-00:00:00      2  alloc znode[44-45]
   long         up 7-00:00:00      6   idle znode[39-40,43,46,54-55]
##############################################################################





################################## Slurm upgrade ##########################################
I've successfully (re)installed munge and finally have slurmctld installed and working on hpcgate. Here are my notes on what I did.


--------------------
Server
--------------------

hpcgate.us.rdlabs.hpecorp.net



--------------------
Previous Slurm versions
--------------------

[root@hpcgate]# sinfo -V
slurm 17.11.3-2

[root@hpcgate]# /optlocal/aislurm/bin/sinfo -V
slurm 17.11.3-2

[root@hpcgate]# /optlocal/slurm/bin/sinfo -V
slurm 16.05.5



--------------------
References
--------------------

Munge install guide:
https://github.com/dun/munge/wiki/Installation-Guide

Slurm install guide:
https://slurm.schedmd.com/quickstart_admin.html

Slurm.conf reference:
https://slurm.schedmd.com/slurm.conf.html

Slurm downloads:
https://www.schedmd.com/archives.php

hpcgate's slurm.conf:
/apps/aislurm/etc/slurm.conf

Launch slurmctld manually on hpcgate:
/optlocal/aislurm/sbin/slurmctld -f /apps/aislurm/etc/slurm.conf



--------------------
Backups before beginning
--------------------

[root@hpcgate]# mkdir /tmp/slurm_backups

[root@hpcgate]# grep StateSaveLocation /apps/aislurm/etc/slurm.conf
StateSaveLocation=/var/tmp

[root@hpcgate]# cp -a /var/tmp /tmp/slurm_backups



--------------------
Install munge, must be done first
--------------------

[root@hpcgate apps]# mkdir /apps/mungerpmbuild

[root@hpcgate apps]# cd /apps/mungerpmbuild

[root@hpcgate mungerpmbuild]# wget https://github.com/dun/munge/releases/download/munge-0.5.14/munge-0.5.14.tar.xz

[root@hpcgate mungerpmbuild]# yum install bzip2-devel # required for next step

[root@hpcgate mungerpmbuild]# rpmbuild -tb --without verify munge-0.5.14.tar.xz
...
Wrote: /root/rpmbuild/RPMS/x86_64/munge-0.5.14-1.el7.x86_64.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/munge-devel-0.5.14-1.el7.x86_64.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/munge-libs-0.5.14-1.el7.x86_64.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/munge-debuginfo-0.5.14-1.el7.x86_64.rpm
...

[root@hpcgate]# rpm -ivh /root/rpmbuild/RPMS/x86_64/munge-0.5.14-1.el7.x86_64.rpm /root/rpmbuild/RPMS/x86_64/munge-devel-0.5.14-1.el7.x86_64.rpm /root/rpmbuild/RPMS/x86_64/munge-libs-0.5.14-1.el7.x86_64.rpm


# Create munge key
sudo -u munge /usr/sbin/mungekey -v

# Start munge
systemctl enable munge
systemctl start munge
systemctl status munge


# To troubleshoot, review the munge log:
/var/log/munge/munged.log



The following are additional steps that may or may not be needed. These were from my first attempt at installing munge, but I'm not sure if they are actually required using the method documented above. However, "just in case" I wanted to be sure to document them:

vi /etc/sysconfig/munge
OPTIONS="--num-threads=10"

mkdir /etc/munge
chmod 0700 /etc/munge

mkdir /var/lib/munge
chmod 0700 /var/lib/munge

mkdir /var/log/munge
chmod 0700 /var/log/munge

mkdir /run/munge
chmod 0755 /run/munge

chown munge:munge /usr/local/etc/munge/
chown munge:munge /usr/local/var/lib/munge
chown munge:munge /usr/local/var/log/munge/munged.log
chown munge:munge /usr/local/var/run/munge/

# Create munge user
groupadd -g 1001 munge
useradd  -m -c "MUNGE" -d /var/lib/munge -u 1001 -g munge  -s /sbin/nologin munge




--------------------
Install SLURM 19.05.8
--------------------

Newer versions of slurm (20.11.5 and 20.02.6) require python3, which is not available on hpcgate. Therefore, I've installed 19.05.8.

[root@hpcgate]# python -V
Python 2.7.5


[root@hpcgate apps]# cd /apps

[root@hpcgate apps]# wget https://download.schedmd.com/slurm/slurm-19.05.8.tar.bz2

[root@hpcgate apps]# tar -xaf slurm-19.05.8.tar.bz2

[root@hpcgate apps]# cd /apps/slurm-19.05.8

[root@hpcgate slurm-20.11.5]# ./configure --prefix=/apps/aislurm --with-munge=/usr/local/bin/munge

[root@hpcgate slurm-19.05.8]# make


# Stop the running slurm and move it's directories aside
[root@hpcgate slurm-19.05.8]# systemctl stop slurmctld

[root@hpcgate slurm-19.05.8]# mv /apps/aislurm /apps/aislurm-17.11.3-2

[root@hpcgate slurm-19.05.8]# cp -a /optlocal/aislurm /optlocal/aislurm-17.11.3-2


# Install the new slurm
[root@hpcgate slurm-19.05.8]# make install


# Copy config files from the old slurm to the new one
[root@hpcgate slurm-19.05.8]# cd /apps/aislurm
[root@hpcgate aislurm]# cp -pr ../aislurm-17.11.3-2/etc/ .
[root@hpcgate aislurm]# cp -pr ../aislurm-17.11.3-2/log/ .
[root@hpcgate aislurm]# cp -pr ../aislurm-17.11.3-2/custom/ .


# Overwrite some symlinks in the 19.05.8 version of /optlocal/aislurm
[root@hpcgate]# cd /optlocal/aislurm
[root@hpcgate aislurm]# ln -sf /apps/aislurm/bin bin
[root@hpcgate aislurm]# ln -sf /apps/aislurm/lib lib
[root@hpcgate aislurm]# ln -sf /apps/aislurm/sbin sbin



# Modify slurm.conf
[root@hpcgate aislurm]# vi /apps/aislurm/etc/slurm.conf

# Comment out this line:
#CryptoType=crypto/openssl

# Add this line:
CredType=cred/munge

# Change this line:
AuthType=auth/none
# to:
AuthType=auth/munge


# Start slurmctld
ldconfig -n /apps/aislurm/lib
systemctl enable slurmctld
systemctl start slurmctld
systemctl status slurmctld



# To troubleshoot, review the slurmctld log:
/apps/aislurm/log/slurmctld.log
#################################################################################3
