lsblk
blkid

# find out top biggest directories
du -a /home | sort -n -r | head -n 5

# turn on led light for the drives
lsmcli local-disk-ident-led-on --path /dev/mapper/ S4P3E019

# search for the biggest file
du -a / 2>/dev/null | sort -n -r | head -n 20 

#get serial number of hard drive sda
udevadm info --query=all --name=/dev/sda | grep -i ID_SERIAL

# The df command is primarily intended to report file system disk space usage. 
df

# The lsblk command with out any argument will print out the block devices in a tree format.
lsblk
lsblk -o NAME,FSTYPE,SIZE,MOUNTPOINT,LABEL               #with arguments

# blkid will print out several different attributes about the block devices.
blkid

# lshw, which can print out detailed information about your hardware
lshw | grep -i [drive | storage | disk]
lshw -class disk

# fdisk is a popular command mostly used to manipulate the partition table. 
fdisk -l

# list the size of a partition, list the partitions on a device, check the partitions on a device, and (very dangerous) repartition a device.
sfdisk

# cfdisk is a partition utility
cfdisk

# parted is another partition table manipulation utility; -l command line option to print out the devices or disks and all its info.
parted -l

# print out the contents of the /proc/partitions/ folder. This will print out all known devices and partitions in the system.
cat /proc/partitions





#######################################################
#mdadm is a Linux utility used to manage and monitor software RAID devices

####### nodes not connecting mounting point: /dev/md0 #########
#Accessible filesystems at /etc/fstab
#check filesystem for /dev/md0
  cat /etc/fstab
#check the status of management device (md)
  cat /proc/mdstat
#stop management device admin at device id: md0
  mdadm --stop /dev/md0
##check the status of management device (md) again
  cat /proc/mdstat
#management device admin create raid 0 on device id called md0 at device sda5 partition
  mdadm --create --verbose /dev/md0 --level=0 --raid-devices=2 /dev/sda5 /dev/sdb5
#scanning and copying the detail information from management device admin into mdadm.conf file
  mdadm --detail --scan > /etc/mdadm.conf
#build the NFS point md0
  mdadm --assemble /dev/md0
#making file system into xfs for device md0
  mkfs.xfs -f /dev/md0
  reboot
#######################################################





########################################################
#To stop a raid
  mdadm --stop /dev/md0
# Here's how to zero the superblocks.
#Just do that for each drive in the array.
  mdadm --zero-superblock /dev/sd[a1]
#######################################################





#######################################################
# create raid

# Wipe out the partation /dev/sda
    dd if=/dev/zero of=/dev/sda bs=1024 count=10000 
    dd if=/dev/zero of=/dev/sdc bs=1024 count=10000
# Shows that the disk is wiped
    parted /dev/sda p
    parted /dev/sdc p
    reboot
# Makes label "gpt"
    parted /dev/sda mklabel gpt 
    parted /dev/sdc mklabel gpt 
# Make disk (parted start at 1049 Bits)
    parted /dev/sda mkpart primary xfs 1049 -1
    parted /dev/sdc mkpart primary xfs 1049 -1

# Create a raid between both sda and sdc
    mdadm --create /dev/md0 -l raid0 -n2 /dev/sda1 /dev/sdc1

# Shows the software array
    cat /proc/mdstat
############ Output #########
        Personalities : [raid0]
        md0 : active raid0 sdc1[1] sda1[0]
            937435136 blocks super 1.2 512k chunks

        unused devices: <none>
############ End ############

# Shows details of the soft array
    mdadm --detail /dev/md0
########### Output #############
Personalities : [raid0]
md0 : active raid0 sdc1[1] sda1[0]
      937435136 blocks super 1.2 512k chunks

unused devices: <none>
[root@harmonian001 ~]# mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Tue Apr 30 13:25:25 2019
        Raid Level : raid0
        Array Size : 937435136 (894.01 GiB 959.93 GB)
      Raid Devices : 2
     Total Devices : 2
       Persistence : Superblock is persistent

      Update Time : Tue Apr 30 13:25:25 2019
             State : clean
    Active Devices : 2
   Working Devices : 2
    Failed Devices : 0
     Spare Devices : 0

        Chunk Size : 512K

Consistency Policy : none

              Name : harmonian001:0  (local to host harmonian001)
              UUID : aa4566f5:e67f3995:d37a141e:4ff6ab1f
            Events : 0

    Number   Major   Minor   RaidDevice State
       0       8        1        0      active sync   /dev/sda1
       1       8       33        1      active sync   /dev/sdc1
################# End #################

# create the array conf file 
    mdadm --detail --scan --verbose >> /etc/mdadm.conf 
# cat /etc/mdadm.conf
############# Output ################
ARRAY /dev/md0 level=raid0 num-devices=2 metadata=1.2 name=harmonian001:0 UUID=aa4566f5:e67f3995:d37a141e:4ff6ab1f
   devices=/dev/sda1,/dev/sdc1
############# End ###############   

# checking raid device name
    lsblk
# Now create a file system on md0
    mkfs.xfs -f /dev/md0
# adding label to raid drive
    xfs_admin -L "lvol" /dev/md0
######### output ######
writing all SBs
new label = "lvol"
####### End ######
# ADD NEW DIR
    mkdir /lvol
    chmod 777 /lvol
    
# adding entry to /etc/fstab
    echo 'LABEL=lvol                          /lvol           xfs rw,defaults 0 0' >> /etc/fstab
# or
    vi /etc/fstab
# add this to the end of the file:
    LABEL=lvol                          /lvol           xfs rw,defaults 0 0
    
# mount raid drive
    mount /dev/md0 /lvol
#######################################################



#######################################################
https://gist.github.com/plepe/52ecc9f18efb32c68d18

command	            description
cat /proc/mdstat	show status of all raids
mdadm --detail /dev/md0	detailed status of raid md0
mdadm --create /dev/md0 -n2 -l1 /dev/sda1 /dev/sdb1	new raid /dev/md0 with 2 disks, raid level 1 on /dev/sda1 and /dev/sda2
mdadm --fail /dev/md0 /dev/sda1 ; mdadm --remove /dev/md0 /dev/sda1	remove /dev/sda1 from /dev/md0
mdadm --add /dev/md0 /dev/sda1	add /dev/sda1 to /dev/md0
mdadm --grow /dev/md0 -n3	use 3 disks in raid /dev/md0 (e.g. add an additional disk, so a damaged drive can be removed later-on)
mdadm --assemble /dev/md0	Assemble /dev/md0 (e.g. when running live system)
mdadm --detail --scan >> /etc/mdadm/mdadm.conf	Update list of arrays in /etc/mdadm/mdadm.conf ; you should remove old list by hand first!
mdadm --examine /dev/sda1	What is this disk / partition?
sysctl -w dev.raid.speed_limit_min=10000	Set minimum raid rebuilding speed to 10000 kiB/s (default 1000)
sfdisk -d /dev/sdX | sfdisk /dev/sdY	Copy partition table from sdX to sdY (MBR only)
sgdisk /dev/sdX -R /dev/sdY ; sgdisk -G /dev/sdY	Copy partition table from sdX to sdY (GPT)
To boot a machine even with a degraded array, modify /etc/initramfs-tools/conf.d/mdadm and run update-initramfs -c -kall (Use with caution!)
lvm
Glossary:

pv: physical device (e.g. /dev/md0 or /dev/sda1)
vg: volume group (consists of 1 or more pvs, contains lvs); has a name (e.g. lvm)
lv: logical volume (has a name which defines its path, e.g. /dev/lvm/root which equals dev/mapper/lvm-root)
command	description
pvcreate /dev/md0	initializes /dev/md0 as phys device for a volume group
vgcreate lvm /dev/md0	create volume group lvm with phys device /dev/md0
lvcreate -L30G -nroot lvm ; mkfs.ext4 /dev/lvm/root	create logical volume root, sized 30G in volume group lvm; format with ext4
lvextend -L60G -nroot lvm ; resize2fs /dev/lvm/root	extend /dev/lvm/root to 60G; also resize file system to new size
pvs, vgs, lvs	show short info about pv, vg and lv
pvdisplay, vgdisplay, lvdisplay	show long info
pvscan /dev/md0	scan disks for physical volumes (e.g. when running live system)
vgextend lvm /dev/md1	add phys device /dev/md1 to volume group lvm (need pvcreate first!)
pvmove /dev/md0 ; vgreduce lvm /dev/md0	move all logical volumes from /dev/md0 and remove phys device from volume group
#################################################################3
