# you can virutally console to a node using either the two commands.
	tail -F /var/log/consoles/hostname
	console [hostname]
############################################################
hpchpcm /thai_notes


# hpcm list of console:
  /var/log/console

# console into a node
  console [hostname]

# For HPCM all package/application is installed from the repo diretory or enviornment module
# installing packages is a bad idea (via yum/apt-get) because it’s a tmpfs image (once it reboots the image is returned to a pristine state)
# list of all RPM package
# change directory and pick the node arch
  cd /opt/clmgr/repos/distro/
    cd rhel7.6-aarch64     or     cd rhel7.6-x86_64
  cd /Packages
  
# installing conrep hp scripting tool
  cinstallman --yum-node --node cronusn003 --repo-group rhel76-x86_64-mel-4.5 install hp-scripting-tools

# reboot
  ipmiwrapper [hostname] power reset
 
# show all OS images 
  cinstallman --show-images
  
# assigning an image and kernel to a node
  cinstallman --assign-image --image rhel7.6-mlnx-4.5-1-nv-410.79-hpc --kernel 3.10.0.-957.el7.x86_64 --node zeusn042
# on the next boot install assigned image
  cinstallman --next-boot image --node zeusn042

# install repo package
# for the package name drop the .rpm
  cinstallman --yum-node --node [hostname] --repo-group [repo name] install [package_name]

# show node and image
  cinstallman --show-nodes | grep [keyword] 
  
# cloning and image to modify  
  cinstallman --create-image --clone --source [image] --image [name_of_clone]

# add the extra kernel params
  cadmin --set-kernel-extra-params --image [imagename] "cgroup_disable=memory intel_pstate=disable"

# show kernel params
  cadmin --show-kernel-extra-params --image [imagename]

# 
  chroot [image]
  rpm -qa | grep [keyword]

#  show repo of the group
  crepo --show-groups
  
# remove nodes from pbs
 qmgr
# you will enter qmgr interface use the d for delete and n for node
 d n [hostname]
 quit

#################################### Adding node to sales op ###################################
# set sales op per node
	qmgr -c "set node [node] resources_available.sales_op = [none]"
# list sales op
	pbsnodes -av | egrep "^c|^l|^a|^r|^z|^h|sales_op" | more
#################################### End ###################################












################### Setting/Config nodes on hpchpcm #################



#######################################
# DO NOT TRY TO DISCOVER MORE THAN ONE NODE AT A TIME.

# Add node to array (mpi crosstalk) before discovering them:
# on hpchpcm
	cd /opt/clmgr/image/images/generic-customizations/etc/array/
	vi arrayd.conf
# add node to file

# copy arrayd file into all image : /opt/clmgr/image/images
	cp arrayd.conf 	[image directory]/etc/array/
	# example: cp arrayd.conf 	/opt/clmgr/image/images/rhel7.6-mlnx-zeus-hpc/etc/array/

# Copying array file to hpchou-login which runs the file
	scp arrayd.conf root@hpchou-login:/etc/array/
	
# On hpchou-login
	systemctl restart array
	array uptime
# Check to see if the nodes show up

# Copying array file to all the nodes
	pdcp  -w apollon[034-041] /etc/array/arrayd.conf /etc/array/arrayd.conf 
	pdcp  -w artemisn[001-004,007,009-014] /etc/array/arrayd.conf /etc/array/arrayd.conf 
	pdcp  -w harmonian[001-005,007-010] /etc/array/arrayd.conf /etc/array/arrayd.conf 
	pdcp  -w kratosn[01-05] /etc/array/arrayd.conf /etc/array/arrayd.conf 
	pdcp  -w leton[001-002] /etc/array/arrayd.conf /etc/array/arrayd.conf 
	pdcp  -w rhean[025-045,047-048] /etc/array/arrayd.conf /etc/array/arrayd.conf 
	pdcp  -w tibern[01-08] /etc/array/arrayd.conf /etc/array/arrayd.conf 
	pdcp  -w zeusn[039-042] /etc/array/arrayd.conf /etc/array/arrayd.conf 

# on hpchpcm
ipmiwrapper [hostname] power off

# discover by node by default it will install rhel7.6

# on hpchpcm check to see if the image is loading
cinstallman --show-nodes --boot-info --node kratosn01

# go to hpchou-login check to see when the named is gone, when it is restart named
ps -edf | grep named
systemctl restart named

# check the nic interface if its wrong from the configure file you must update the file and rediscover

# on hpchpcm
ipmiwrapper kratosn01 power off

# discover node while its off
discover --node [service_number] --configfile /root/[new_cluster_config_file from above]

# go to hpchou-login check to see when the named is gone, when it is restart named
ps -edf | grep named
systemctl restart named

# on hpchpcm console to the node
# the default password for the base image is cmdefault
console kratosn01

# check all the drive, there is two drive the first will be the OS so parted the second drive so the discover will only see one blank drive
lsblk
parted /dev/sda p
parted /dev/sdb p
parted /dev/sdb mklabel gpt
parted
mkpart primary xfs 1049kb -1
quit

# or kill one drive with
sgdisk --zap-all /dev/sd[x]

# create /lvol on second drive
mkfs.xfs -f /dev/sdb1
xfs_admin -L "lvol" /dev/sdb1

# on the node check the boot order
efibootmgr
efibootmgr -o "0000"
	eth PXE IPv4
	SGI slot chooser
	
			# then go to hpchpcm clone image
			cinstallman --create-image --clone --source rhel7.6-mlnx-AMD-hpc --image rhel7.-mlnx-kratos-hpc --repo-group leto

			# then create a repo group, show leto group to check the repo that is installed on it then copy it over to the new created group
			crepo --show-group leto
			crepo --add-group kratos [leto repo]

			# add cluster for PSB file
			cd images/generic-customization/etc/array
			vi array.service

			# copy the array into the image
			cp arrayd.con /opt/clmgr/image/images/rhel7.-mlnx-kratos-hpc/etc/array

			# edit and copy over roundrobin and after.local into the image directory

# now its time to assign the new image to the discoved node
cinstallman --show-image
cinstallman --assign-image --image rhel7.6-mlnx-kratos-hpc --node kratosn01 --kernel (the kernel listed next to the image from show image)
cinstallman --next-boot image --node kratosn02



# on hpchou-log
# creating node for pbs using qmgr
for NODE in 'seq 1 4' ; do qmgr -c "create node kratosn0${NODE}" ; done
pbsnodes -av | egrep '^kratos'

# define sales_op and cluster
for NODE in 'seq 1 4' ; do qmgr -c "set node kratosn0${NODE} resources_available.cluster = kratos" ; done
for NODE in 'seq 1 4' ; do qmgr -c "set node kratosn0${NODE} resources_available.sales_op = none" ; done

# create the frequency queues
cd /
sh fqgen.sh $(sh [nodename] cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_frequencies)

# If the output is good pipe it to qmgr to add it to PBS queues
sh fqgen.sh $(ssh [nodename] cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_frequencies) | qmgr

# Then update the /etc/motd on hpchou-login
vi /etc/motd

# On hpchpcm
# To remove all entry of a node	
	cadmin --db-purge --node [service_number]	
	
# hpchpcm switch OPA to IB but cronusn does not boot correctly run cmd below:
    pdsh -w cronusn[001-144] /admin/gbc/cronusfixme

############################################# Discovering new node into HPCHPCM / PBS #############################################
# Notes:
# by default all computing nodes when discover process will have rhel7.6

# Clone the basic image and rename it
	cinstallman --create-image --clone --source rhel7.6-mlnx-4.5-1-nv-410.79-hpc --image [new_image_name]

# set up ilo and bios for hpchpcm
# get Mac address of node nic & ilo

# check the image for its kernel params
	cadmin --show-kernel-extra-params --image rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc

# find the last service id number from /etc/hosts by grep
# copy a config file and edit to add server hostname and both MAC address
	cd /root on hpchpcm
	cp [basic_config_file] [new_cluster_config_file]
	vi [new_cluster_config_file]

# copy the nis file and rename it
	cd /opt/clmgr/image/scripts/post-install
	cp 97rhel7.6-mlnx-4.5-1-hpc.hpc.nis 97rhel7.6-mlnx-4.5-1-AMD-hpc.nis
# only use when you copy the whole config file
	discover --node [service_number] --configfile /root/[new_cluster_config_file from above]
# only use discover --all if you only have the new nodes only in the discovery file.
	discover --all --configfile /root/[new_cluster_config_file from above]
	
# on your node add a raid
# add label for lvol
# Now create a file system on md0 (or md127)
# Check what label was given to the device
	lsblk
    mkfs.xfs -f /dev/md0
# adding label to raid drive
    xfs_admin -L "lvol_m2" /dev/md0

	######### output ######
	writing all SBs
	new label = "lvol"
	####### End ######

# adding entry to /etc/fstab
    echo 'LABEL=lvol                          /lvol           xfs rw,defaults 0 0' >> /etc/fstab
# or
    vi /etc/fstab
# add this to the end of the file:
    LABEL=lvol                          /lvol           xfs rw,defaults 0 0

# adding cluster to roundrobin and after.local script
# edit roundrobin.sh and after.local
# copy the for loop to add said hostname and edit lvol label name
	cd /opt/clmgr/image/images/generic-customizations/etc
	vi roundrobin.sh
	vi after.local.sh
# copy roundrobin and after.local into image dirctory
	cp roundrobin.sh /opt/clmgr/image/images/[new_cluster_config_file]/etc/opt/sgi/conf.d/
	cp after.local /opt/clmgr/image/images/[new_cluster_config_file]/etc/init.d/

# check boot order
	efibootmgr
# rerange boot order
	efibootmgr -o [PXE IPv4],[uefi shell]# http://rusty.chf.rdlabs.hpecorp.net/wiki/index.php/PBS_Installation_in_SMC_14.x#Create_the_Node_List_on_PBS_Server

# on hpchost3
# que manager to create node
	qmgr -c "create node leton002"
# list node to see if creation was successful
	pbsnodes -av | grep leton
# create sales op and cluster for said node
	qmgr -c "set node leton002 resources_available.sales_op = none"
	qmgr -c "set node leton002 resources_available.cluster = leto"
# check sales op
	pbsnodes -av | egrep "^a|^h|^l|^z|^r|c|sales_op"
# run frequency queues script on node
	sh fqgen.sh `ssh leton002 cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_frequencies`
# If the output looks good, pipe it to qmgr
	sh fqgen.sh `ssh r01n01 cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_frequencies` | qmgr
# errors are ok
# add new cluster to message of the day
	vi /etc/motd

# submit a job to test PBS
	 qsub -I -l select=2:ncpus=20:mpiprocs=10:cluster=leto:sales_op=none -l walltime=8:00:00 -l place=scatter:excl
	


################################################### End #########################################################















############################ create an RPM build from a runfile into the current OS and kernel ##################### 
# on the computing node
# change directory:
    [root@hpchpcm /]# cd /admin/hpcm/NVIDIA
# Download nvidia driver generic Linux 64-bit (non-distro)
# https://www.nvidia.com/Download/index.aspx?lang=en-us​
# use the sgi script to build the runfile into a RPM
[root@zeusn039 NVIDIA]# ./sgi-package-nvidia [nvidia_driver_runfile]
 
[root@hpchpcm ~]# cinstallman --show-nodes | grep zeusn
zeusn039     service187   rhel7.6-mlnx-4.5-1-nv-410.79-hpc 3.10.0-957.el7.x86_64
zeusn040     service188   rhel7.6-mlnx-4.5-1-nv-410.79-hpc 3.10.0-957.el7.x86_64
zeusn041     service189   rhel7.6-mlnx-4.5-1-nv-410.79-hpc 3.10.0-957.el7.x86_64
zeusn042     service190   rhel7.6-mlnx-4.5-1-nv-410.79-hpc 3.10.0-957.el7.x86_64

[root@hpchpcm ~]# cinstallman --show-images
Image Name                               BT VCS Compat_Distro
rhel7.6                                  0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-hpc                              0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-mlnx-4.5-1-hpc                   0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-mlnx-4.5-1-nv-410.79-hpc         0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-op.1090-hpc                      0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel76-aarch-hpc                         0  1   rhel7
        0-rescue-b390d4426b554a588b2ce2dee6582274
        4.14.0-115.el7a.aarch64

# Clone the image
    [root@hpchpcm ~]# cinstallman --create-image --clone --source rhel7.6-mlnx-4.5-1-nv-410.79-hpc --image rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc
# Show knernel params
    [root@hpchpcm ~]# cadmin --show-kernel-distro-params --image rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc
# Add the extra kernel params
    [root@hpchpcm ~]# cadmin --set-kernel-extra-params --image rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc "cgroup_disable=memory intel_pstate=disable"

# Chroot into the image
# Image location
    [root@hpchpcm /]# cd /opt/clmgr/image/images
    [root@hpchpcm /]# chroot rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc
    [root@hpchpcm /]# rpm -qa | grep nvidia
        nvidia_peer_memory-1.0-8.x86_64
        nvidia-410.79-rhel7.x86_64

# Repo groups
    [root@hpchpcm images]# crepo --show-groups
        Repo Groups:
        rhel76-x86_64-mel-4.5-nv
        rhel76-x86_64-OPA-10.9
        rhel76_arm
        rhel76-x86_64-mel-4.5

# Create the repo
    [root@hpchpcm ~]# mkdir  /opt/clmgr/repos/nvidia-418.40.04-rpm-mln
    [root@hpchpcm ~]# cp nvidia-418.40.04-rhel7.x86_64.rpm /opt/clmgr/repos/nvidia-418.40.04-rpm-mln
    [root@hpchpcm ~]# crepo --add /opt/clmgr/repos/nvidia-418.40.04-rpm-mln --custom nvidia-418.40.04-rpm-mln

# Add the repo to the repo group.
    [root@hpchpcm ~]# crepo --add-group rhel76-x86_64-mel-4.5-nv  nvidia-418.40.04-rpm-mln

# Now we install the package into our image
    [root@hpchpcm ~]# cinstallman --yum-image --image rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc --repo-group rhel76-x86_64-mel-4.5-nv install nvidia-410.79-rhel7.x86_64

# Update post-install settings.
    [root@hpchpcm ~]# cd /opt/clmgr/image/scripts/post-install/

    [root@hpchpcm post-install]# ls
00all.run_cluster_configuration                 97rhel76-aarch-hpc.hpc.nis        97rhel7.6-mlnx-4.5-1-nv-410.79-hpc.hpc.nis  README
50all.create_filesystem_for_reserved_partition  97rhel7.6-hpc.hpc.nis             97rhel7.6-op.1090-hpc.hpc.nis
95all.monitord_rebooted                         97rhel7.6-mlnx-4.5-1-hpc.hpc.nis  99all.harmless_example_script

    [root@hpchpcm post-install]# cp 97rhel7.6-mlnx-4.5-1-nv-410.79-hpc.hpc.nis 97rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc.hpc.nis

# Now we are going to image a node with our new image.
# We assign the image to the node.

    [root@hpchpcm ~]# cinstallman --show-images
Image Name                               BT VCS Compat_Distro
rhel7.6                                  0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-hpc                              0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-mlnx-4.5-1-hpc                   0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-mlnx-4.5-1-nv-410.79-hpc         0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc      0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel7.6-op.1090-hpc                      0  1   rhel7
        0-rescue-f547d73dfd8e4d398e476733bc1aa090
        3.10.0-957.el7.x86_64
rhel76-aarch-hpc                         0  1   rhel7
        0-rescue-b390d4426b554a588b2ce2dee6582274
        4.14.0-115.el7a.aarch64

    [root@hpchpcm ~]# cinstallman --assign-image --image  rhel7.6-mlnx-4.5-1-nv-418.40.04-hpc --kernel 3.10.0-957.el7.x86_64 --node zeusn039

# Need to tell the node to image on the next boot.
    [root@hpchpcm ~]# cinstallman --next-boot image --node zeusn039


    [root@zeusn039 ~]# efibootmgr
BootCurrent: 0012
Timeout: 0 seconds
BootOrder: 0012,0010,0013,0011,0014,0016,0017,0019,0018,0015,001E,0020,001B,001D,001F,001C,000A,000B,000F,0021,0001,001A,0023,0002,0003,0004,0005,0006,0007,0008,0009,0000,000C,000D,000E,0022
Boot0000* System Utilities
Boot0001* Embedded UEFI Shell
Boot0002  Diagnose Error
Boot0003  Intelligent Provisioning
Boot0004  Boot Menu
Boot0005  Network Boot
Boot0006  View Integrated Management Log
Boot0007  HTTP Boot
Boot0008  PXE Boot
Boot0009  Embedded Diagnostics
Boot000A* Generic USB Boot
Boot000B* Embedded SATA Port 7 HDD : VR000150GWEPP
Boot000C* NVMe Drive Port 1B Box 1 Bay 7 : NVM Express Controller - ZC502264-MK001600KWDUN-C013CE8
Boot000D* NVMe Drive Port 1B Box 1 Bay 8 : NVM Express Controller - ZC502285-MK001600KWDUN-C013D2D
Boot000E* Slot 1
Boot000F* Embedded RAID 1 : HPE Smart Array P816i-a SR Gen10 - 894.2 GiB, RAID0 Logical Drive 1(Target:0, Lun:0)
Boot0010* Embedded LOM 1 Port 1 : HPE Ethernet 1Gb 4-port 331i Adapter - NIC (HTTP(S) IPv4)
Boot0011* Embedded LOM 1 Port 1 : HPE Ethernet 1Gb 4-port 331i Adapter - NIC (HTTP(S) IPv6)
Boot0012* Embedded LOM 1 Port 1 : HPE Ethernet 1Gb 4-port 331i Adapter - NIC (PXE IPv4)
Boot0013* Embedded LOM 1 Port 1 : HPE Ethernet 1Gb 4-port 331i Adapter - NIC (PXE IPv6)
Boot0014* Slot 11 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (HTTP(S) IPv4)
Boot0015* Slot 11 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (HTTP(S) IPv6)
Boot0016* Slot 11 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (PXE IPv4)
Boot0017* Slot 9 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (HTTP(S) IPv4)
Boot0018* Slot 9 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (HTTP(S) IPv6)
Boot0019* Slot 9 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (PXE IPv4)
Boot001A* @ CentOS Linux release 7.4.1708 (Core)
Boot001B* Slot 12 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (HTTP(S) IPv4)
Boot001C* Slot 12 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (HTTP(S) IPv6)
Boot001D* Slot 12 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (PXE IPv4)
Boot001E* Slot 10 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (HTTP(S) IPv4)
Boot001F* Slot 10 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (HTTP(S) IPv6)
Boot0020* Slot 10 Port 1 : HPE InfiniBand EDR 100Gb 1-port 841QSFP28 Adapter - HCA (PXE IPv4)
Boot0021* SGI Slot Chooser
Boot0022* Embedded RAID 1 : HPE Smart Array P816i-a SR Gen10 - Size:931.5 GiB Port:1I Bay:1 Box:1
Boot0023* Red Hat Enterprise Linux

# To Change the boot order
    [root@zeusn039 ~]# efibootmgr -o 0000,0020

# Reboot the node and watch it boot onto the new image.
####################################### End #########################################










############################ SET UP OF DISK FOR "harmonian" Nodes ################################

# Take node off PBS
    qstat -an                                               # show all nodes in PBS
    pbsnodes -o harmonina001 -C "provisioning raid"         # taking node off PBS with comments
    pbsnodes -l                                             # list offline nodes

# List what drives are installed on the node
    lsscsi
######### Output ##########
[0:0:0:0]    disk    ATA      VR000480GWFMD    HPG8  /dev/sda
[1:0:0:0]    disk    ATA      MB4000GVYZA      HPG3  /dev/sdb
[2:0:0:0]    disk    ATA      VR000480GWFMD    HPG8  /dev/sdc
######### End ########
# or use list block
    lsblk
########### Output #########
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sdb       8:16   0   3.7T  0 disk
└─sdb1    8:17   0   3.7T  0 part
sr2      11:2    1  1024M  0 rom
sr0      11:0    1  1024M  0 rom
sdc       8:32   0 447.1G  0 disk
sda       8:0    0 447.1G  0 disk
├─sda2    8:2    0     2G  0 part [SWAP]
├─sda32 259:3    0 222.1G  0 part
├─sda22 259:1    0   150M  0 part
├─sda12   8:12   0   300M  0 part
├─sda3    8:3    0    50M  0 part
├─sda1    8:1    0    50M  0 part
├─sda31 259:2    0 222.1G  0 part /
├─sda21 259:0    0   150M  0 part /boot/efi
└─sda11   8:11   0   300M  0 part /boot
sr3      11:3    1  1024M  0 rom
sr1      11:1    1  1024M  0 rom
############## End ############


# shows drives as parted and size
    parted /dev/sda p
    parted /dev/sdc p
# Setup image to be forced on /dev/sdb *use this command on hpchpcm not on the computing node*
    cinstallman --next-boot image --node {NODE} --force-disk /dev/sda
    reboot

# Shows boot order
    efibootmgr
########## Output #########
BootCurrent: 0001
Timeout: 1 seconds
BootOrder: 0001,0002,0003,0004,0005,0006,0007
Boot0001* RedHat Boot Manager
Boot0002* UEFI: Network Card
Boot0003* UEFI: PXE IP4 Mellanox Network Adapter - 04:09:73:B8:2A:28
Boot0004* UEFI: PXE IP6 Mellanox Network Adapter - 04:09:73:B8:2A:28
Boot0005* UEFI: PXE IP4 Mellanox Network Adapter - 04:09:73:B8:2A:29
Boot0006* UEFI: PXE IP6 Mellanox Network Adapter - 04:09:73:B8:2A:29
Boot0007* UEFI: Built-in EFI Shell
############# End ##########

# Change boot order
    efibootmgr -o 0003,0007
########## Output #########
BootCurrent: 0001
Timeout: 1 seconds
BootOrder: 0003,0007
Boot0001* RedHat Boot Manager
Boot0002* UEFI: Network Card
Boot0003* UEFI: PXE IP4 Mellanox Network Adapter - 04:09:73:B8:2A:28
Boot0004* UEFI: PXE IP6 Mellanox Network Adapter - 04:09:73:B8:2A:28
Boot0005* UEFI: PXE IP4 Mellanox Network Adapter - 04:09:73:B8:2A:29
Boot0006* UEFI: PXE IP6 Mellanox Network Adapter - 04:09:73:B8:2A:29
Boot0007* UEFI: Built-in EFI Shell
############# End ##########

# Wipe out the partation /dev/sda
    dd if=/dev/zero of=/dev/sda bs=1024 count=10000 
    dd if=/dev/zero of=/dev/sdc bs=1024 count=10000
# Shows that the disk is wiped
    parted /dev/sda p
    parted /dev/sdc p
    reboot
# Makes label "gpt"
    parted /dev/sda mklabel gpt 
    parted /dev/sdc mklabel gpt 

# Make disk (parted start at 1049 Bits
# parted /dev/sda mkpart primary xfs 1049 -1
# parted /dev/sdc mkpart primary xfs 1049 -1

# Use parted tool 
    parted 
    select
    /dev/sda
    mkpart primary xfs 1049k -1
# Then do the same for /dev/sdc
    select
    /dev/sdc
    mkpart primary xfs 1049k -1

# p "to ptrint"
# quit  "to end the tool"

# Create a raid between both sda and sdc
    mdadm --create /dev/md0 -l raid0 -n2 /dev/sda1 /dev/sdc1

# Shows the software array
    cat /proc/mdstat
############ Output #########
        Personalities : [raid0]
        md0 : active raid0 sdc1[1] sda1[0]
            937435136 blocks super 1.2 512k chunks

        unused devices: <none>
############ End ############

# Shows details of the soft array
    mdadm --detail /dev/md0
########### Output #############
Personalities : [raid0]
md0 : active raid0 sdc1[1] sda1[0]
      937435136 blocks super 1.2 512k chunks

unused devices: <none>
[root@harmonian001 ~]# mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Tue Apr 30 13:25:25 2019
        Raid Level : raid0
        Array Size : 937435136 (894.01 GiB 959.93 GB)
      Raid Devices : 2
     Total Devices : 2
       Persistence : Superblock is persistent

      Update Time : Tue Apr 30 13:25:25 2019
             State : clean
    Active Devices : 2
   Working Devices : 2
    Failed Devices : 0
     Spare Devices : 0

        Chunk Size : 512K

Consistency Policy : none

              Name : harmonian001:0  (local to host harmonian001)
              UUID : aa4566f5:e67f3995:d37a141e:4ff6ab1f
            Events : 0

    Number   Major   Minor   RaidDevice State
       0       8        1        0      active sync   /dev/sda1
       1       8       33        1      active sync   /dev/sdc1
################# End #################

# create the array conf file 
    mdadm --detail --scan --verbose >> /etc/mdadm.conf 
# cat /etc/mdadm.conf
############# Output ################
ARRAY /dev/md0 level=raid0 num-devices=2 metadata=1.2 name=harmonian001:0 UUID=aa4566f5:e67f3995:d37a141e:4ff6ab1f
   devices=/dev/sda1,/dev/sdc1
############# End ###############   

# checking raid device name
    lsblk
# Now create a file system on md0
    mkfs.xfs -f /dev/md0
# adding label to raid drive
    xfs_admin -L "lvol_m2" /dev/md0
######### output ######
writing all SBs
new label = "lvol"
####### End ######
# ADD NEW DIR
    mkdir /lvol
    chmod 777 /lvol
    
# adding entry to /etc/fstab
    echo 'LABEL=lvol                          /lvol           xfs rw,defaults 0 0' >> /etc/fstab
# or
    vi /etc/fstab
# add this to the end of the file:
    LABEL=lvol                          /lvol           xfs rw,defaults 0 0
    
# mount raid drive
    mount /dev/md0 /lvol
# reboot node and check to see if the raid | /lvol is mounted and not booted into emergency mode
    reboot
    df | grep lvol
# resume node on PBS
    pbsnodes -r harmonian001
#################################################### End ################################################










#################################### Adding node to sales op ###################################
# set sales op per node
	qmgr –c “set node [node] resources_available.sales_op = [sales op name]”
# list sales op
	pbsnodes -av | egrep "^c|^l|^a|^r|^z|^h|sales_op" | more
#################################### End ###################################







##################### Updating Sudoer #########################
# Modifying Sudoer file for Houston lab & updating HPCM nodes.

# On hpchost3 change directory to sudoers.d
	[root@hpchost3 ~]# cd /etc/sudoers.d
	[root@hpchost3 sudoers.d]# ls
	bcfree-lite  houston

# Edit the houston file to update sudoers permission access.
# You can use vi or nano text editor.
	[root@hpchost3 sudoers.d]# vi houston

# Change directory to /etc/dsh/group.
# This directoy holds all cluster's computing nodes in a group alias.
	[root@hpchost3 sudoers.d]# cd /etc/dsh/group
# List all the current group alias
	[root@hpchost3 group]# ls
	apollo-gen10  artemis  cronus  harmonia  leto  rhea  zeus

# From here we will copy the houston file to a group of hosts in parallel.
	[root@hpchost3 group]# pdcp -g [group_node] /etc/sudoers.d/houston /etc/sudoers.d/houston
###############################################################





########################## Removing node from LR0 to LR4 ####################################
For future reference, here are some steps to keep DBs, PBS, and array services up to date when moving nodes. These steps use kratosn04/05 as an example:

ssh hpchou-login;
pbsnodes -o kratosn04 -C "Moving to PLAB"
pbsnodes -o kratosn05 -C "Moving to PLAB"
ssh kratosn04
shutdown -h now
ssh kratosn05
shutdown -h now

ssh hpchpcm;
cadmin --db-purge --node kratosn04
cadmin --db-purge --node kratosn05

ssh hpchou-login;
qmgr -c "delete node kratosn04"
qmgr -c "delete node kratosn05"
cd /etc/array
cp arrayd.conf arrayd-backups/arrayd.conf.06042020
vi arrayd.conf # and update node list
cp arrayd.conf /admin/pkh/

# steps to copy the updated arrayd.conf everywhere 
pdsh  -w apollon[034-041] 'cp /admin/pkh/arrayd.conf /etc/array/arrayd.conf'
pdsh  -w artemisn[001-004,007,009-014] 'cp /admin/pkh/arrayd.conf /etc/array/arrayd.conf' 
pdsh  -w harmonian[001-005,007-010] 'cp /admin/pkh/arrayd.conf /etc/array/arrayd.conf'
pdsh  -w kratosn[01-05] 'cp /admin/pkh/arrayd.conf /etc/array/arrayd.conf'
pdsh  -w leton[001-002] 'cp /admin/pkh/arrayd.conf /etc/array/arrayd.conf'
pdsh  -w rhean[025-045,047-048] 'cp /admin/pkh/arrayd.conf /etc/array/arrayd.conf' 
pdsh  -w tibern[01-16] 'cp /admin/pkh/arrayd.conf /etc/array/arrayd.conf' 
pdsh  -w zeusn[039-042] 'cp /admin/pkh/arrayd.conf /etc/array/arrayd.conf' 

# on hpchou-login 
# steps to restart array.service
systemctl restart array.service 
pdsh  -w apollon[034-041] 'systemctl restart array.service'
pdsh  -w artemisn[001-004,007,009-014] 'systemctl restart array.service'
pdsh  -w harmonian[001-005,007-010] 'systemctl restart array.service'
pdsh  -w kratosn[01-05] 'systemctl restart array.service'
pdsh  -w leton[001-002] 'systemctl restart array.service'
pdsh  -w rhean[025-045,047-048] 'systemctl restart array.service'
pdsh  -w tibern[01-16] 'systemctl restart array.service'
pdsh  -w zeusn[039-042] 'systemctl restart array.service'



# on hpchpcm now copy arrayd.conf to all the images used 
unalias cp 
cp /admin/pkh/arrayd.conf /opt/clmgr/image/images/rhel7.6-mlnx-apollo-hpc/etc/array/
cp /admin/pkh/arrayd.conf /opt/clmgr/image/images/rhel7.6-mlnx-gen9-hpc/etc/array/
cp /admin/pkh/arrayd.conf /opt/clmgr/image/images/rhel76-aarch-hpc/etc/array/
cp /admin/pkh/arrayd.conf /opt/clmgr/image/images/rhel7.6-mlnx-kratos-hpc/etc/array/
cp /admin/pkh/arrayd.conf /opt/clmgr/image/images/rhel7.6-mlnx-AMD-hpc/etc/array/
cp /admin/pkh/arrayd.conf /opt/clmgr/image/images/rhel7.6-mlnx-rhea-hpc/etc/array/
cp /admin/pkh/arrayd.conf /opt/clmgr/image/images/rhel7.6-mlnx-zeus-hpc/etc/array/
cp /admin/pkh/arrayd.conf /opt/clmgr/image/images/rhel7.6-hpchou-hpc/etc/array/

# restart namd on hpchou-login
systemctl restart named
#################################################################


# For standalone on hpchpcm, discover the node but do not assign an image using skip provision then install the ISO
discover --node 266 --skip-provision --configfile /root/hpchpcm.config


################################################################
qstat looking up cpu frequencies

[root@hpchou-login ~]# qstat -Q | grep f29 | sort
f2900                0     0 yes yes     0     0     0     0     0     0 Exec
f2900noHT            0     0 yes yes     0     0     0     0     0     0 Exec
f2900THP             0     0 yes yes     0     0     0     0     0     0 Exec
f2900THPnoHT         0     0 yes yes     0     0     0     0     0     0 Exec

