#diagnostic test
  dcgmi diag -r 1

#lx270D Gen10 GPU slot location for Nvidia Tesla v100 SXM2
#b2:00.0 - slot 1
#b3:00.0 - slot 3
#89:00.0 - slot 2
#8a:00.0 - slot 4
#3a:00.0 - slot 5
#3b:00.0 - slot 7
#15:00.0 - slot 6
#16:00.0 - slot 8

#using Bus ID to indecate phyical slot location
  lspci -vs [bus_ID] | grep -i slot

#list number of GPU | name | UUID | PCI busID
  nvidia-xconfig --query-gpu-info
  
#show cuda version
  cat /usr/local/cuda/version.txt 

#command to see gpu location
  nvidia-smi

nvidia-smi -q | grep Driver
#show driver version

#locate serial number for GPU
  nvidia-smi -q | grep Serial
  
#get bios info
  nvidia-smi -q | grep VBIOS

#shorter list
  nvidia-smi -L
  
#list all nvidia processes
  lsof /dev/nvidia*
# OR
  top | grep nvidia
#kill process
  kill -9 [PID]
  
#nvidia persistenced
  systemctl status nvidia-persistenced
  systemctl start nvidia-persistenced
  systemctl stop nvidia-persistenced
  systemctl reload nvidia-persistenced
  
# Enabled persistence mode for GPU <target gpu>.  
# to enable all GPU
  nvidia-smi -i <target gpu> -pm ENABLED
  nvidia-smi -pm 1

# To view current persistence mode using nvidia-smi:
  nvidia-smi -i <target gpu> - q
  #############################################
    ==============NVSMI LOG==============

    Timestamp                           : ----
    Driver Version                      : ----

    Attached GPUs                       : ----
    GPU 0000:01:00.0
        Product Name                    : ----
        Display Mode                    : ----
        Display Active                  : ----
        Persistence Mode                : Enabled
        Accounting Mode                 : ----
  #############################################
  
#checking zeusn nodes GPU count Nvidia Tesla V100 SXM2 
 sudo /admin/nodes/scripts/gpuCount

#list all rpm that are installed
  rpm -qa | grep nvidia
  
#slot location with serial number matching  
  nvidia-smi -q | grep 'GPU\|Serial' 

#nv_peer_mem module
  service nv_peer_mem start
  service nv_peer_mem status
  service nv_peer_mem stop
  
#list all module with nvidia
  lsmod | grep nvidia
#unloading nvidia module   
  rmmod nvidia-drm
  rmmod nvidia-modeset
  rmmod nvidia-uvm
  rmmod nvidia-nvidia
  
# To Turn off the ECC RAM, just do a
  nvidia-smi -g 0 --ecc-config=0
# (repeat with -g x for each GPU ID)
# To Turn back on ECC RAM, just do
  nvidia-smi -g 0 --ecc-config=1
# (repeat with -g x for each GPU ID)  

# to reset ECC errors to 0 for all GPUs
nvidia-smi --reset-ecc-errors=0
##########
nvidia-smi --reset-ecc-errors=0
Reset volatile ECC errors to zero for GPU 00000000:1B:00.0.
Reset volatile ECC errors to zero for GPU 00000000:29:00.0.
Reset volatile ECC errors to zero for GPU 00000000:45:00.0.
Reset volatile ECC errors to zero for GPU 00000000:4E:00.0.
Reset volatile ECC errors to zero for GPU 00000001:1B:00.0.
Reset volatile ECC errors to zero for GPU 00000001:24:00.0.
Reset volatile ECC errors to zero for GPU 00000001:45:00.0.
Reset volatile ECC errors to zero for GPU 00000001:4E:00.0.
All done.
##########
  
#quitting nvidia cuda
  nvidia-cuda-mps-control -quit

#install nvidia tensorflow
docker pull nvcr.io/nvidia/tensorflow-18.09-py3
docker images
nvidia-docker run -ti --rm nvcr.io/nvidia/tensorflow-18.09-py3 /bin/bash
#ti terminal interactive rm remove the container instaint
exit

################################################
download nvidia driver for v100
rpm -i [driver]
yum clean all
yum install -y nvidia
yum install -y nvidia-driver.x86_64
yum install -y nvidia-driver-cuda.x86_64

download cuda 10 toolkit
rpm -i [cuda 10]
yum install -y cuda
yum install -y cuda-libraries-10-0

###############################################
disable nouveau kernal driver to install nvidia driver

RHEL:
  echo "blacklist nouveau" >/lib/modprobe.d/blacklist.conf
        or /etc/modprobe.d/nvidia-installer-disable-nouveau.conf
  mv /boot/initramfs-`uname -r`.img /boot/initramfs-`uname -r`.img.bkp
  dracut -v /boot/initramfs-$(uname -r).img $(uname -r)
  reboot

-----------

Ubuntu:
create file: 
  nano /etc/modprobe.d/blacklist-nouveau.conf
with the following contents:
  blacklist nouveau
  options nouveau modeset=0

Regenerate the kernel initramfs for Ubuntu:
  sudo update-initramfs -u
  reboot
###############################################




##############################################

If any of the GPU clocks is running at a slower speed, one or more of the above Clocks Throttle Reasons will be marked as active. If HW Slowdown was active, as this would most likely indicate a power or cooling issue.

root@aixl675dn05:~# nvidia-smi -q -d PERFORMANCE

==============NVSMI LOG==============

Timestamp                                 : Thu Jul 22 17:09:29 2021
Driver Version                            : 460.27.04
CUDA Version                              : 11.2

Attached GPUs                             : 8
GPU 00000000:07:00.0
    Performance State                     : P0
    Clocks Throttle Reasons
        Idle                              : Not Active
        Applications Clocks Setting       : Not Active
        SW Power Cap                      : Not Active
        HW Slowdown                       : Active
            HW Thermal Slowdown           : Not Active
            HW Power Brake Slowdown       : Active
        Sync Boost                        : Not Active
        SW Thermal Slowdown               : Not Active
        Display Clock Setting             : Not Active



root@aixl675dn05:~# nvidia-smi -i 0 -q -d MEMORY,UTILIZATION,POWER,CLOCK,COMPUTE

==============NVSMI LOG==============

Timestamp                                 : Thu Jul 22 17:14:38 2021
Driver Version                            : 460.27.04
CUDA Version                              : 11.2

Attached GPUs                             : 8
GPU 00000000:07:00.0
    FB Memory Usage
        Total                             : 81251 MiB
        Used                              : 0 MiB
        Free                              : 81251 MiB
    BAR1 Memory Usage
        Total                             : 131072 MiB
        Used                              : 1 MiB
        Free                              : 131071 MiB
    Compute Mode                          : Default
    Utilization
        Gpu                               : 0 %
        Memory                            : 0 %
        Encoder                           : 0 %
        Decoder                           : 0 %
    Power Readings
        Power Management                  : Supported
        Power Draw                        : 64.73 W
        Power Limit                       : 400.00 W
        Default Power Limit               : 400.00 W
        Enforced Power Limit              : 400.00 W
        Min Power Limit                   : 100.00 W
        Max Power Limit                   : 400.00 W
    Power Samples
        Duration                          : 18446744073709.51 sec
        Number of Samples                 : 119
        Max                               : 65.06 W
        Min                               : 64.39 W
        Avg                               : 0.00 W
        
###############################################





###############################################
If nvidia-smi only see a few GPU but lspci sees all GPU, then you will need to turn on dip switch 6 power on then power off and turn off
dip switch 6.

root@znode48:/# dmesg | grep -i nvidia
[    6.074635] nvidia: loading out-of-tree module taints kernel.
[    6.074642] nvidia: module license 'NVIDIA' taints kernel.
[    6.152950] nvidia: module verification failed: signature and/or required key missing - tainting kernel
[    6.278102] nvidia-nvlink: Nvlink Core is being initialized, major device number 238
[    6.290955] nvidia 0000:15:00.0: enabling device (0140 -> 0142)
[    6.303148] NVRM: This PCI I/O region assigned to your NVIDIA device is invalid:
[    6.341774] nvidia: probe of 0000:15:00.0 failed with error -1
[    6.354991] nvidia 0000:16:00.0: enabling device (0140 -> 0142)
[    6.392783] NVRM: This PCI I/O region assigned to your NVIDIA device is invalid:
[    6.432248] nvidia: probe of 0000:16:00.0 failed with error -1
[    6.458125] nvidia 0000:3a:00.0: enabling device (0140 -> 0142)
[    6.538874] nvidia 0000:3b:00.0: enabling device (0140 -> 0142)
[    6.594313] nvidia 0000:89:00.0: enabling device (0140 -> 0142)
[    6.594370] NVRM: This PCI I/O region assigned to your NVIDIA device is invalid:
[    6.594379] nvidia: probe of 0000:89:00.0 failed with error -1
[    6.594392] nvidia 0000:8a:00.0: enabling device (0140 -> 0142)
[    6.594423] NVRM: This PCI I/O region assigned to your NVIDIA device is invalid:
[    6.594428] nvidia: probe of 0000:8a:00.0 failed with error -1
[    6.594448] nvidia 0000:b2:00.0: enabling device (0140 -> 0142)
[    6.594480] NVRM: This PCI I/O region assigned to your NVIDIA device is invalid:
[    6.594485] nvidia: probe of 0000:b2:00.0 failed with error -1
[    6.594495] nvidia 0000:b3:00.0: enabling device (0140 -> 0142)
[    6.594524] NVRM: This PCI I/O region assigned to your NVIDIA device is invalid:
[    6.594528] nvidia: probe of 0000:b3:00.0 failed with error -1
[    6.594550] NVRM: The NVIDIA probe routine failed for 6 device(s).
[    6.594551] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  440.33.01  Wed Nov 13 00:00:22 UTC 2019
[    6.601982] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms  440.33.01  Tue Nov 12 23:43:11 UTC 2019
[    6.712905] [drm] [nvidia-drm] [GPU ID 0x00003a00] Loading driver
[    6.713416] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:3a:00.0 on minor 1
[    6.713988] [drm] [nvidia-drm] [GPU ID 0x00003b00] Loading driver
[    6.714452] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:3b:00.0 on minor 2
[   12.642474] nvidia-uvm: Loaded the UVM driver, major device number 511.
##############################################
