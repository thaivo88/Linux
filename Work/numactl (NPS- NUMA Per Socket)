NAME

numactl – manage NUMA policy configuration

DESCRIPTION

The numactl command can be used to assign NUMA policies to processes/threads, run commands with a given NUMA policy, and query information about NUMA policies 
on running processes.
numactl requires a target to modify or query. The target may be specified as a command, process id or a thread id. Using --get the target's NUMA policy may be queried. 
Using --set the target's NUMA policy may be queried. If no target is specified, numactl operates on itself. Not all combinations of operations and targets are supported. 
For example, you may not set the id of an existing set or query and launch a command at the same time.

Each process and thread has a NUMA policy. By default the policy is NONE. If a thread policy is NONE, then the policy will fall back to the process. 
If the process policy is NONE, then the policy will fall back to the system default. The policy may be queried by using --get.

The options are as follows:
--cpudomain domain , -c domain
 	Set the given CPU scheduling policy. Constrain the object (tid, pid, command) to run on CPUs that belong to the given domain.
--get -, -g
 	Retrieve the NUMA policy for the given thread or process id.
--set -, -s
 	Set the NUMA policy for the given thread or process id.
--memdomain domain , -m domain
 	Constrain the object (tid, pid, command) to the given domain. This is only valid for fixed-domain and fixed-domain-rr. It must not be set for other policies.
--mempolicy policy , -l policy
 	Set the given memory allocation policy. Valid policies are none, rr, fixed-domain, fixed-domain-rr, first-touch, and first-touch-rr. A memdomain argument is required for fixed-domain and fixed-domain-rr.
--pid pid , -p pid
 	Operate on the given pid.
--tid tid , -t tid
 	Operate on the given tid.

SYNOPSIS

numactl [-l policy] [-m domain] [-c domain] cmd ...
numactl -g [-p pid] [-t tid]
numactl -s [-l policy] [-m domain] [-c domain] [-p pid] [-t tid]

###########################################################################################

# install numa control
  dnf install numactl

# Show inventory of available nodes on the system
  numactl --hardware
# or  
  numactl -H
# or  
  ssh [hostname] numactl --hardware
    ##########Output##########
    available: 2 nodes (0-1)
    node 0 cpus: 0 1 2 3 4 5
    node 0 size: 16383 MB
    node 0 free: 9762 MB
    node 1 cpus: 6 7 8 9 10 11
    node 1 size: 16384 MB
    node 1 free: 4575 MB
    node distances:
    node   0   1 
      0:  10  13 
      1:  13  10 
    ###########End###########


# Show NUMA policy settings of the current process
  numactl -show
    ##########Output##########  
    policy: default
    preferred node: current
    physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 
    cpubind: 0 1 
    nodebind: 0 1 
    membind: 0 1 
    ###########End###########

# Show how much memory is free on each node
 numactl -H | grep free
    ##########Output##########  
    node 0 free: 9769 MB
    node 1 free: 4566 MB
    ###########End###########

# Control application use particular CPU node
# You can use numactl to control how your program uses CPU node, controlling which CPUs are used to execute a program.
  numactl --cpunodebind=<node> ls
# or
  numactl -N node
  
# Control application use particular CPU
# You can use numactl to control how your program uses CPU, controlling which CPU are used to execute a program 
  numactl --physcpubind=<cpu> ls
# or
  numactl -C <cpu>  
# This accepts cpu numbers as shown in the processor fields of /proc/cpuinfo. Thus, if hyper threading is enabled, the virtual cores representing a hyper thread, 
# not a hardware thread will show up as well.
# You can also specify range of CPUs for an application, the following command runs  myapp  on  cpus 0-4 and 8-12 of the current cpuset.
  numactl  --physcpubind=+0-4,8-12  myapp  arguments    
  
# Control application use local memory
# You can use numactl to control how your program uses memory quite precisely, simply using numactl to require that the program uses only local memory can result 
# in much improved performance. This can be achieved by using numactl with flag -l
  numactl --physcpubind=0 --localalloc ls
# The program can use only local memory, which is less than the total memory, and will fail if it tries to allocate more memory when the local memory is already full.
  
# Control application use preferred node and memory
# Preferably allocate memory on node, but if memory cannot be allocated  there  fall  back  to  other nodes.  This option takes only a single node number.
  numactl --preferred=<node> ls
  
# Interleave memory for some application
# Run big database with its memory interleaved on all CPUs.
  numactl --interleave=all bigdatabase arguments
  
# Run network server on specified network device with its memory in the same node
# The command below run network-server on the node of network device eth0 with its memory also in the same node.
  numactl  --cpunodebind=netdev:eth0  --membind=netdev:eth0   

#################
# Using the vm.zone_reclaim_mode kernel parameter to control node overflow
# The vm.zone_reclaim_mode kernel parameter controls allocation behavior when a memory allocation request is about to overflow to
# another node. When vm.zone_reclaim_mode is disabled or zero, the kernel simply overflows to the next node in the target node’s zone list.

# When vm.zone_reclaim_mode is enabled, the kernel attempts to free up or reclaim pages from the target node’s memory before going
# off-node for the allocation

# set zone_reclaim_mode 
[root@kratosn05 /]# sysctl -w vm.zone_reclaim_mode=3
vm.zone_reclaim_mode = 3
[root@kratosn05 /]# cat /proc/sys/vm/zone_reclaim_mode
3
#################
